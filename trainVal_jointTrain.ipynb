{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import net_sphere\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import datetime,sys\n",
    "from numpy.random import randint\n",
    "import torchvision.models as models\n",
    "from calculateEvaluationCCC import calculateCCC\n",
    "import glob\n",
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "# torch.cuda.CUDA_VISIBLE_DEVICES = 2\n",
    "\n",
    "\n",
    "# Define parameters\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = 0\n",
    "# lr = 0.01\n",
    "# bs = 32\n",
    "# n_epoch = 20\n",
    "# lr_steps = [7,14]\n",
    "lr = 0.001\n",
    "bs = 6\n",
    "n_epoch = 30\n",
    "lr_steps = [7,14,21,28]\n",
    "gpu_id = [0,1]\n",
    "\n",
    "gd = 20 # clip gradient\n",
    "eval_freq = 3\n",
    "print_freq = 100\n",
    "num_worker = 4\n",
    "num_seg = 16\n",
    "num_stft = 4\n",
    "flag_biLSTM = True\n",
    "\n",
    "\n",
    "train_list_path = './data/OMG_Aligned/train_list_lstm.txt'\n",
    "val_list_path = './data/OMG_Aligned/val_list_lstm.txt'\n",
    "Vmodel_path = './new_model/model_lstm_73.pth'\n",
    "Amodel_path = './model/426_best.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Video Network (VNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sphereface = getattr(net_sphere,'sphere20a')()\n",
    "# sphereface.load_state_dict(torch.load(model_path))\n",
    "sphereface.feature = True # remove the last fc layer because we need to use LSTM first\n",
    "\n",
    "class VNet(torch.nn.Module):\n",
    "    def __init__(self, sphereface, feature=True):\n",
    "        super(VNet, self).__init__()\n",
    "        self.sphereface = sphereface\n",
    "        self.linear = torch.nn.Linear(512,2)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.feature = feature\n",
    "        self.avgPool = torch.nn.AvgPool2d((num_seg,1), stride=1)\n",
    "        self.LSTM = torch.nn.LSTM(512, 512, 1, batch_first = True, dropout=0.2, bidirectional=flag_biLSTM)  # Input dim, hidden dim, num_layer\n",
    "        for name, param in self.LSTM.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                torch.nn.init.constant(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                torch.nn.init.orthogonal(param)\n",
    "        \n",
    "    def sequentialLSTM(self, input, hidden=None):\n",
    "\n",
    "        input_lstm = input.view([-1,num_seg, input.shape[1]])\n",
    "        batch_size = input_lstm.shape[0]\n",
    "        feature_size = input_lstm.shape[2]\n",
    "\n",
    "        self.LSTM.flatten_parameters()\n",
    "            \n",
    "        output_lstm, hidden = self.LSTM(input_lstm)\n",
    "        if flag_biLSTM:\n",
    "             output_lstm = output_lstm.contiguous().view(batch_size, output_lstm.size(1), 2, -1).sum(2).view(batch_size, output_lstm.size(1), -1) \n",
    "\n",
    "        output_lstm = output_lstm.view(batch_size,1,num_seg,-1)\n",
    "        out = self.avgPool(output_lstm)\n",
    "        out = out.view(batch_size,-1)\n",
    "        return out\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.sphereface(x)\n",
    "        x = self.sequentialLSTM(x)\n",
    "        if self.feature == True: return x\n",
    "\n",
    "        x = self.linear(x)\n",
    "        x = self.tanh(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model_v = VNet(sphereface)\n",
    "\n",
    "model_v.load_state_dict(torch.load(Vmodel_path))\n",
    "# if use_cuda:\n",
    "#     model.cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model_v.feature = True\n",
    "# # print model_v\n",
    "\n",
    "# t = torch.autograd.Variable(torch.randn(16,3,112,96))\n",
    "\n",
    "# o = model_v(t)\n",
    "# print o.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Audio Network (ANet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = models.vgg16(pretrained=False).features\n",
    "\n",
    "removed = list(vgg.children())[1:]\n",
    "vgg = torch.nn.Sequential(*removed)\n",
    "\n",
    "# We modified the first layer of vgg16\n",
    "vgg_modified = torch.nn.Sequential(torch.nn.Conv2d(2,64,3),vgg)\n",
    "\n",
    "class ANet(torch.nn.Module):\n",
    "    def __init__(self, vgg,feature=True):\n",
    "        super(ANet, self).__init__()\n",
    "        self.vgg = vgg\n",
    "        self.fc1 = torch.nn.Linear(512*7*9,4096)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "        self.fc2 = torch.nn.Linear(4096,512)\n",
    "        self.fc3 = torch.nn.Linear(512,2)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.feature = feature\n",
    "    def forward(self, x):\n",
    "        x = self.vgg(x)\n",
    "        x = x.view([-1,512*7*9])\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.dropout(self.relu(self.fc2(x)))\n",
    "        if self.feature == True: return x \n",
    "        \n",
    "        x = self.tanh(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "model_a = ANet(vgg_modified)\n",
    "\n",
    "\n",
    "# t = torch.autograd.Variable(torch.randn(1,2,257,300))\n",
    "# o = model_a(t)\n",
    "# print o.shape\n",
    "model_a.load_state_dict(torch.load(Amodel_path))\n",
    "# for p in model.parameters():\n",
    "#     p.requires_grad = True\n",
    "\n",
    "# if use_cuda:\n",
    "#     model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint Audio and Video Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bs = 2\n",
    "class AVNet(torch.nn.Module):\n",
    "    def __init__(self, vnet,anet):\n",
    "        super(AVNet, self).__init__()\n",
    "        self.vnet = vnet\n",
    "        self.anet = anet\n",
    "        self.avgPool = torch.nn.AvgPool2d((num_stft,1), stride=1)\n",
    "        self.fc = torch.nn.Linear(1024,2)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        \n",
    "    def forward(self, xi,xs):\n",
    "        xi = self.vnet(xi) \n",
    "        xs = self.anet(xs)\n",
    "        xs = xs.view((-1,1,num_stft,512))\n",
    "        xs = self.avgPool(xs)\n",
    "        xs = xs.view(-1,512)\n",
    "        \n",
    "        \n",
    "        x = torch.cat((xi, xs), 1)\n",
    "        x = self.tanh(self.fc(x))\n",
    "        return x\n",
    "\n",
    "xi = torch.autograd.Variable(torch.randn(32,3,96,112))\n",
    "xs = torch.autograd.Variable(torch.randn(8,2,257,300))\n",
    "\n",
    "model = AVNet(model_v, model_a)\n",
    "\n",
    "\n",
    "if use_cuda:\n",
    "#     model = torch.nn.DataParallel(model, device_ids=gpu_id).cuda()\n",
    "    model.cuda()\n",
    "\n",
    "# o = model(xi,xs)\n",
    "# print o.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OMGDataset(Dataset):\n",
    "    \"\"\"OMG dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, txt_file, base_path_v, base_path_a, transform=None):\n",
    "        self.base_path_v = base_path_v\n",
    "        self.base_path_a = base_path_a\n",
    "        self.data = pd.read_csv(txt_file, sep=\" \", header=None)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vid = self.data.iloc[idx,0]\n",
    "        utter = self.data.iloc[idx,1]\n",
    "        img_list = self.data.iloc[idx,-1]\n",
    "        img_list = img_list.split(',')[:-1]\n",
    "        img_list = map(int, img_list)\n",
    "        \n",
    "        num_frames = len(img_list)\n",
    "        # inspired by TSN's pytorch code\n",
    "        average_duration = num_frames // num_seg\n",
    "        if num_frames>num_seg:\n",
    "            offsets = np.multiply(list(range(num_seg)), average_duration) + randint(average_duration, size=num_seg)\n",
    "        else:\n",
    "            tick = num_frames / float(num_seg)\n",
    "            offsets = np.array([int(tick / 2.0 + tick * x) for x in range(num_seg)])\n",
    "\n",
    "        final_list = [img_list[i] for i in offsets]\n",
    "        \n",
    "        # stack images within a video in the depth dimension\n",
    "        for i,ind in enumerate(final_list):\n",
    "            image = io.imread(self.base_path_v+'%s/%s/%d.jpg'%(vid,utter,ind)).astype(np.float32)\n",
    "            image = torch.from_numpy(((image - 127.5)/128).transpose(2,0,1))\n",
    "            if i==0:\n",
    "                images = image\n",
    "            else:\n",
    "                images = torch.cat((images,image), 0)\n",
    "        \n",
    "        \n",
    "        # stft data acquisition\n",
    "        stft_path = self.base_path_a+vid+'/'+utter\n",
    "        stfts_count = len(glob.glob1(stft_path,\"*.npy\"))\n",
    "        stft_list_all = range(stfts_count)\n",
    "        \n",
    "        average_duration = stfts_count // num_stft\n",
    "        if stfts_count>num_stft:\n",
    "            offsets = np.multiply(list(range(num_stft)), average_duration) + randint(average_duration, size=num_stft)\n",
    "        else:\n",
    "            tick = stfts_count / float(num_stft)\n",
    "            offsets = np.array([int(tick / 2.0 + tick * x) for x in range(num_stft)])\n",
    "        \n",
    "        stft_list = [stft_list_all[i] for i in offsets]\n",
    "        \n",
    "        for i,ind in enumerate(stft_list):\n",
    "            \n",
    "            stft = np.load(stft_path+'/%d.npy'%ind).astype(np.float32)\n",
    "            max_val = max(np.abs(np.max(stft)),np.abs(np.min(stft)))\n",
    "            mean_val = np.mean(stft)\n",
    "            stft = torch.from_numpy(((stft - mean_val)/max_val).transpose(2,0,1))\n",
    "            if i==0:\n",
    "                stfts = stft\n",
    "            else:\n",
    "                stfts = torch.cat((stfts,stft), 0)\n",
    "                \n",
    "        \n",
    "        \n",
    "        label = torch.from_numpy(np.array([self.data.iloc[idx,2], self.data.iloc[idx,3]]).astype(np.float32))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return (images, stfts, label, (vid,utter))\n",
    "    \n",
    "train_loader = DataLoader(OMGDataset(train_list_path,'./data/OMG_Aligned/Train/', './data/STFT/Train/'), \n",
    "                          batch_size=bs, shuffle=True, num_workers=num_worker)\n",
    "val_loader = DataLoader(OMGDataset(val_list_path,'./data/OMG_Aligned/Val/', './data/STFT/Val/'), \n",
    "                        batch_size=bs, shuffle=False, num_workers=num_worker)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printoneline(*argv):\n",
    "    s = ''\n",
    "    for arg in argv: s += str(arg) + ' '\n",
    "    s = s[:-1]\n",
    "    sys.stdout.write('\\r'+s)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def dt():\n",
    "    return datetime.datetime.now().strftime('%H:%M:%S')\n",
    "\n",
    "def save_model(model,filename):\n",
    "    state = model.state_dict()\n",
    "#     for key in state: state[key] = state[key].clone().cpu()\n",
    "    torch.save(state, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pearsonr(outputs, targets):\n",
    "    vx = outputs - torch.mean(outputs)\n",
    "    vy = targets - torch.mean(targets)\n",
    "    rho = torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)))  # use Pearson correlation\n",
    "    return rho\n",
    "\n",
    "def calCCC(out, tar, rho):\n",
    "    true_mean = torch.mean(tar)\n",
    "    true_variance = torch.var(tar)\n",
    "    pred_mean = torch.mean(out)\n",
    "    pred_variance = torch.var(out)\n",
    "    std_predictions = torch.std(out)\n",
    "    std_gt = torch.std(tar)\n",
    "    \n",
    "    ccc = 2 * rho * std_gt * std_predictions / (\n",
    "        std_predictions ** 2 + std_gt ** 2 +\n",
    "        (pred_mean - true_mean) ** 2)\n",
    "    \n",
    "    return ccc\n",
    "\n",
    "def calLoss(outputs, targets):\n",
    "    out_a = outputs[:,0]\n",
    "    out_v = outputs[:,1]\n",
    "    tar_a = targets[:,0]\n",
    "    tar_v = targets[:,1]\n",
    "    \n",
    "    rho_a = pearsonr(out_a, tar_a)\n",
    "    rho_v = pearsonr(out_v, tar_v)\n",
    "    \n",
    "    ccc_a = calCCC(out_a,tar_a,rho_a)\n",
    "    ccc_v = calCCC(out_v,tar_v,rho_v)\n",
    "    \n",
    "    ccc_all = -(ccc_a+ccc_v)\n",
    "    return ccc_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batch_idx = 0\n",
    "    \n",
    "    for i, (inputs_v, inputs_a, targets, _) in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if use_cuda:\n",
    "            inputs_v, inputs_a, targets = inputs_v.cuda(), inputs_a.cuda(), targets.cuda(async=True)\n",
    "\n",
    "        inputs_v = torch.autograd.Variable(inputs_v)\n",
    "        inputs_a = torch.autograd.Variable(inputs_a)\n",
    "        targets = torch.autograd.Variable(targets)\n",
    "        \n",
    "        inputs_v = inputs_v.view((-1,3)+inputs_v.size()[-2:])\n",
    "        inputs_a = inputs_a.view((-1,2)+inputs_a.size()[-2:])\n",
    "        \n",
    "        outputs = model(inputs_v, inputs_a)\n",
    "#         print 'here',outputs.shape, targets.shape\n",
    "        \n",
    "        loss = calLoss(outputs, targets)\n",
    "#         loss = criterion(outputs,targets)\n",
    "        \n",
    "#         lossd = loss.data[0]        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #tsn uses clipping gradient\n",
    "        if gd is not None:\n",
    "            total_norm = clip_grad_norm(model.parameters(),gd)\n",
    "            if total_norm > gd:\n",
    "                print('clippling gradient: {} with coef {}'.format(total_norm, gd/total_norm))\n",
    "                \n",
    "        train_loss += loss.data[0]\n",
    "        \n",
    "        if i % print_freq == 0:\n",
    "            printoneline(dt(),'Epoch=%d Loss=%.4f\\n'\n",
    "                % (epoch,train_loss/(batch_idx+1)))\n",
    "        batch_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(val_loader, model, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    err_arou = 0.0\n",
    "    err_vale = 0.0\n",
    "    \n",
    "    out_name = 'results/joint_ccc2_%d.csv'%epoch\n",
    "    txt_result = open(out_name, 'w')\n",
    "    txt_result.write('video,utterance,arousal,valence\\n')\n",
    "    for (inputs_v, inputs_a, targets,(vid, utter)) in val_loader:\n",
    "#         print inputs.shape\n",
    "        if use_cuda:\n",
    "            inputs_v, inputs_a, targets = inputs_v.cuda(), inputs_a.cuda(), targets.cuda()\n",
    "        \n",
    "        inputs_v = torch.autograd.Variable(inputs_v)\n",
    "        inputs_a = torch.autograd.Variable(inputs_a)\n",
    "        targets = torch.autograd.Variable(targets)\n",
    "        \n",
    "        \n",
    "        inputs_v = inputs_v.view((-1,3)+inputs_v.size()[-2:])\n",
    "        inputs_a = inputs_a.view((-1,2)+inputs_a.size()[-2:])\n",
    "        \n",
    "#         try:\n",
    "        outputs = model(inputs_v, inputs_a)\n",
    "#         except:\n",
    "#             print 'here',inputs_v.shape, inputs_a.shape\n",
    "        \n",
    "        for i in range(len(vid)):\n",
    "#             name = img_name[i].replace('/home/m2a03/Work/sphereface_pytorch/data/OMG_Aligned/', '')\n",
    "            out = outputs\n",
    "#             print vid[0], utter[0]\n",
    "            txt_result.write('%s,%s.mp4,%f,%f\\n'%(vid[i], utter[i],out[i][0],out[i][1]))\n",
    "    \n",
    "#     print('MSE of arousal: %f' % (err_arou / len(val_loader.dataset)))\n",
    "#     print('MSE of valence: %f' % (err_vale / len(val_loader.dataset)))\n",
    "#     print('MSE of total: %f' % ((err_arou + err_vale) / len(val_loader.dataset)))\n",
    "    txt_result.close()\n",
    "    \n",
    "    arouCCC, valeCCC = calculateCCC('./results/omg_ValidationVideos.csv',out_name)\n",
    "    return (arouCCC,valeCCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arousal CCC:  0.000729248005929\n",
      "Arousal Cor:  0.0640982519015\n",
      "Arousal MSE:  0.251539628394\n",
      "Valence CCC:  0.0127693192139\n",
      "Valence Cor:  0.251068486037\n",
      "Valence MSE:  0.177341878108\n",
      "Total CCC:    0.0134985672198\n",
      "09:36:04 Epoch=0 Loss=-0.0314\n",
      "clippling gradient: 30.4845379443 with coef 0.656070301493\n",
      "09:37:48 Epoch=0 Loss=-1.1690\n",
      "09:39:33 Epoch=0 Loss=-1.3460\n",
      "09:41:18 Epoch=0 Loss=-1.4052\n",
      "09:43:03 Epoch=0 Loss=-1.4359\n",
      "09:43:13 Epoch=1 Loss=-1.3830\n",
      "09:44:57 Epoch=1 Loss=-1.6308\n",
      "09:46:42 Epoch=1 Loss=-1.6201\n",
      "09:48:27 Epoch=1 Loss=-1.5857\n",
      "09:50:13 Epoch=1 Loss=-1.5784\n",
      "09:50:21 Epoch=2 Loss=-1.6663\n",
      "09:52:06 Epoch=2 Loss=-1.6135\n",
      "09:53:51 Epoch=2 Loss=-1.6102\n",
      "09:55:37 Epoch=2 Loss=-1.5948\n",
      "clippling gradient: 23.6218730917 with coef 0.84667290872\n",
      "09:57:22 Epoch=2 Loss=-1.5883\n",
      "Arousal CCC:  0.214644260994\n",
      "Arousal Cor:  0.230688168067\n",
      "Arousal MSE:  0.0623285554297\n",
      "Valence CCC:  0.447472218107\n",
      "Valence Cor:  0.449795844191\n",
      "Valence MSE:  0.113793266274\n",
      "Total CCC:    0.662116479101\n",
      "09:58:40 Epoch=3 Loss=-1.5894\n",
      "clippling gradient: 20.2007338899 with coef 0.99006303974\n",
      "10:00:24 Epoch=3 Loss=-1.6164\n",
      "10:02:10 Epoch=3 Loss=-1.6160\n",
      "10:03:55 Epoch=3 Loss=-1.6103\n",
      "10:05:41 Epoch=3 Loss=-1.6214\n",
      "10:05:58 Epoch=4 Loss=-1.8550\n",
      "10:07:47 Epoch=4 Loss=-1.6842\n",
      "10:09:33 Epoch=4 Loss=-1.6851\n",
      "clippling gradient: 20.2052635278 with coef 0.989841086334\n",
      "10:11:18 Epoch=4 Loss=-1.6655\n",
      "10:13:03 Epoch=4 Loss=-1.6731\n",
      "10:13:13 Epoch=5 Loss=-1.7991\n",
      "10:14:58 Epoch=5 Loss=-1.6820\n",
      "10:16:43 Epoch=5 Loss=-1.6857\n",
      "clippling gradient: 22.5771975239 with coef 0.885849538182\n",
      "10:18:51 Epoch=5 Loss=-1.6911\n",
      "10:21:39 Epoch=5 Loss=-1.6787\n",
      "Arousal CCC:  0.276348324623\n",
      "Arousal Cor:  0.296713993274\n",
      "Arousal MSE:  0.0547597633827\n",
      "Valence CCC:  0.435533396982\n",
      "Valence Cor:  0.440920778434\n",
      "Valence MSE:  0.117703405918\n",
      "Total CCC:    0.711881721605\n",
      "10:25:57 Epoch=6 Loss=-1.6821\n",
      "clippling gradient: 31.1613459672 with coef 0.641820800072\n",
      "10:28:50 Epoch=6 Loss=-1.6855\n",
      "10:31:42 Epoch=6 Loss=-1.6906\n",
      "10:33:28 Epoch=6 Loss=-1.6778\n",
      "clippling gradient: 25.5215380881 with coef 0.783651828938\n",
      "clippling gradient: 31.2642040744 with coef 0.639709232718\n",
      "clippling gradient: 20.4402702543 with coef 0.978460644168\n",
      "10:35:13 Epoch=6 Loss=-1.6639\n",
      "10:35:22 Epoch=7 Loss=-1.6657\n",
      "10:37:07 Epoch=7 Loss=-1.7436\n",
      "10:39:15 Epoch=7 Loss=-1.7436\n",
      "clippling gradient: 25.4347629082 with coef 0.786325395374\n",
      "10:41:13 Epoch=7 Loss=-1.7498\n",
      "10:43:06 Epoch=7 Loss=-1.7521\n",
      "clippling gradient: 27.8612544303 with coef 0.717842767994\n",
      "10:43:17 Epoch=8 Loss=-1.8828\n",
      "10:45:58 Epoch=8 Loss=-1.7879\n",
      "10:48:36 Epoch=8 Loss=-1.7892\n",
      "clippling gradient: 25.7129570619 with coef 0.777817967489\n",
      "10:51:22 Epoch=8 Loss=-1.7896\n",
      "10:54:00 Epoch=8 Loss=-1.7917\n",
      "Arousal CCC:  0.308436766055\n",
      "Arousal Cor:  0.324124277403\n",
      "Arousal MSE:  0.0540317252716\n",
      "Valence CCC:  0.46622129455\n",
      "Valence Cor:  0.471666411911\n",
      "Valence MSE:  0.108313456847\n",
      "Total CCC:    0.774658060605\n",
      "10:55:49 Epoch=9 Loss=-1.8633\n",
      "10:57:34 Epoch=9 Loss=-1.8075\n",
      "10:59:19 Epoch=9 Loss=-1.8069\n",
      "clippling gradient: 23.8558301284 with coef 0.838369484205\n",
      "clippling gradient: 33.1094381613 with coef 0.604057365835\n",
      "11:01:04 Epoch=9 Loss=-1.7959\n",
      "clippling gradient: 25.7575726366 with coef 0.776470682319\n",
      "11:02:49 Epoch=9 Loss=-1.7958\n",
      "11:03:00 Epoch=10 Loss=-1.9498\n",
      "11:04:45 Epoch=10 Loss=-1.8244\n",
      "clippling gradient: 20.2816168664 with coef 0.986114673782\n",
      "clippling gradient: 23.1310853959 with coef 0.864637333599\n",
      "clippling gradient: 23.7875616197 with coef 0.840775541425\n",
      "11:06:30 Epoch=10 Loss=-1.8172\n",
      "11:08:15 Epoch=10 Loss=-1.8188\n",
      "clippling gradient: 34.5072809749 with coef 0.579587827118\n",
      "clippling gradient: 32.0250262978 with coef 0.624511587095\n",
      "clippling gradient: 20.1278404499 with coef 0.993648575949\n",
      "clippling gradient: 20.6385155812 with coef 0.969061942529\n",
      "11:10:00 Epoch=10 Loss=-1.8064\n",
      "11:10:10 Epoch=11 Loss=-1.8514\n",
      "clippling gradient: 22.134438693 with coef 0.90356933272\n",
      "11:11:55 Epoch=11 Loss=-1.8270\n",
      "clippling gradient: 23.6347626944 with coef 0.846211161862\n",
      "11:13:41 Epoch=11 Loss=-1.8389\n",
      "11:15:26 Epoch=11 Loss=-1.8353\n",
      "clippling gradient: 29.9994880722 with coef 0.666678043034\n",
      "clippling gradient: 32.0829773285 with coef 0.623383540599\n",
      "11:17:11 Epoch=11 Loss=-1.8251\n",
      "Arousal CCC:  0.321458322693\n",
      "Arousal Cor:  0.336103728802\n",
      "Arousal MSE:  0.0514169283943\n",
      "Valence CCC:  0.449060515159\n",
      "Valence Cor:  0.457945284392\n",
      "Valence MSE:  0.107085783699\n",
      "Total CCC:    0.770518837852\n",
      "11:18:55 Epoch=12 Loss=-1.9714\n",
      "clippling gradient: 21.8762009122 with coef 0.914235523811\n",
      "clippling gradient: 22.5525471257 with coef 0.88681778996\n",
      "clippling gradient: 20.3165139492 with coef 0.984420853398\n",
      "11:20:39 Epoch=12 Loss=-1.8159\n",
      "clippling gradient: 27.2342908343 with coef 0.73436830508\n",
      "11:22:24 Epoch=12 Loss=-1.8331\n",
      "clippling gradient: 24.2253911314 with coef 0.825580065622\n",
      "11:24:09 Epoch=12 Loss=-1.8239\n",
      "clippling gradient: 24.9941968837 with coef 0.800185742837\n",
      "clippling gradient: 21.8173002838 with coef 0.91670370485\n",
      "clippling gradient: 22.0881942666 with coef 0.90546106932\n",
      "11:25:54 Epoch=12 Loss=-1.8238\n",
      "11:26:03 Epoch=13 Loss=-1.8851\n",
      "clippling gradient: 26.4168398602 with coef 0.75709282813\n",
      "11:27:49 Epoch=13 Loss=-1.8456\n",
      "clippling gradient: 24.8748505788 with coef 0.804024930185\n",
      "clippling gradient: 25.3352485085 with coef 0.789414005287\n",
      "11:29:34 Epoch=13 Loss=-1.8468\n",
      "clippling gradient: 24.5093873796 with coef 0.816013868082\n",
      "clippling gradient: 24.1546857645 with coef 0.827996695754\n",
      "11:31:19 Epoch=13 Loss=-1.8340\n",
      "11:33:04 Epoch=13 Loss=-1.8345\n",
      "11:33:14 Epoch=14 Loss=-1.8732\n",
      "clippling gradient: 22.7414295672 with coef 0.879452188389\n",
      "clippling gradient: 32.3130034146 with coef 0.618945869668\n",
      "clippling gradient: 20.6010232887 with coef 0.970825561416\n",
      "11:34:59 Epoch=14 Loss=-1.8334\n",
      "clippling gradient: 20.6307221684 with coef 0.969428013076\n",
      "clippling gradient: 22.4900245714 with coef 0.889283154693\n",
      "clippling gradient: 28.1976488518 with coef 0.709278993617\n",
      "11:36:44 Epoch=14 Loss=-1.8382\n",
      "clippling gradient: 20.6854780572 with coef 0.966861870183\n",
      "clippling gradient: 34.7603319895 with coef 0.575368497805\n",
      "11:38:29 Epoch=14 Loss=-1.8386\n",
      "11:40:15 Epoch=14 Loss=-1.8398\n",
      "clippling gradient: 33.3321476881 with coef 0.600021342374\n",
      "Arousal CCC:  0.310118299196\n",
      "Arousal Cor:  0.32553145169\n",
      "Arousal MSE:  0.0517662838961\n",
      "Valence CCC:  0.453611896876\n",
      "Valence Cor:  0.464920511924\n",
      "Valence MSE:  0.104408158451\n",
      "Total CCC:    0.763730196072\n",
      "11:41:50 Epoch=15 Loss=-1.7175\n",
      "clippling gradient: 21.8618172561 with coef 0.914837031418\n",
      "11:43:35 Epoch=15 Loss=-1.8325\n",
      "11:45:20 Epoch=15 Loss=-1.8449\n",
      "clippling gradient: 21.7185789114 with coef 0.920870563474\n",
      "clippling gradient: 20.4070074628 with coef 0.980055504781\n",
      "11:47:05 Epoch=15 Loss=-1.8440\n",
      "clippling gradient: 20.1837621277 with coef 0.990895546303\n",
      "11:48:50 Epoch=15 Loss=-1.8460\n",
      "11:49:00 Epoch=16 Loss=-1.8737\n",
      "clippling gradient: 39.8093694146 with coef 0.502394292954\n",
      "clippling gradient: 31.0600417348 with coef 0.643914138002\n",
      "clippling gradient: 23.1471693773 with coef 0.864036533971\n",
      "11:50:45 Epoch=16 Loss=-1.8188\n",
      "clippling gradient: 50.7686710213 with coef 0.393943737303\n",
      "clippling gradient: 34.0377231629 with coef 0.587583367556\n",
      "clippling gradient: 22.4779838323 with coef 0.889759515322\n",
      "11:52:30 Epoch=16 Loss=-1.8404\n",
      "11:54:16 Epoch=16 Loss=-1.8434\n",
      "11:56:01 Epoch=16 Loss=-1.8452\n",
      "11:56:11 Epoch=17 Loss=-1.8198\n",
      "clippling gradient: 25.1115081992 with coef 0.796447582573\n",
      "clippling gradient: 21.7558001591 with coef 0.919295077806\n",
      "clippling gradient: 21.0170014246 with coef 0.951610536437\n",
      "11:57:56 Epoch=17 Loss=-1.8595\n",
      "11:59:41 Epoch=17 Loss=-1.8504\n",
      "clippling gradient: 22.1603673375 with coef 0.902512115228\n",
      "12:01:27 Epoch=17 Loss=-1.8498\n",
      "12:03:12 Epoch=17 Loss=-1.8518\n",
      "Arousal CCC:  0.316222525499\n",
      "Arousal Cor:  0.332177333376\n",
      "Arousal MSE:  0.0510698019825\n",
      "Valence CCC:  0.455030047362\n",
      "Valence Cor:  0.465207842688\n",
      "Valence MSE:  0.10414716023\n",
      "Total CCC:    0.771252572861\n",
      "12:04:42 Epoch=18 Loss=-1.9340\n",
      "clippling gradient: 37.1580292841 with coef 0.538241677111\n",
      "12:06:28 Epoch=18 Loss=-1.8353\n",
      "clippling gradient: 20.6693540506 with coef 0.967616111807\n",
      "12:08:13 Epoch=18 Loss=-1.8482\n",
      "clippling gradient: 25.0179632838 with coef 0.799425587652\n",
      "12:09:58 Epoch=18 Loss=-1.8427\n",
      "clippling gradient: 26.8108292772 with coef 0.745967228138\n",
      "clippling gradient: 34.2150140768 with coef 0.584538704415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:11:43 Epoch=18 Loss=-1.8460\n",
      "12:11:54 Epoch=19 Loss=-1.8292\n",
      "clippling gradient: 24.7488057294 with coef 0.808119802575\n",
      "12:13:38 Epoch=19 Loss=-1.8651\n",
      "clippling gradient: 21.5657073996 with coef 0.927398282348\n",
      "clippling gradient: 47.8229759397 with coef 0.418209022065\n",
      "12:15:24 Epoch=19 Loss=-1.8569\n",
      "clippling gradient: 35.807898257 with coef 0.558535992715\n",
      "12:17:09 Epoch=19 Loss=-1.8567\n",
      "clippling gradient: 20.4691240439 with coef 0.977081381553\n",
      "clippling gradient: 21.5626914418 with coef 0.927527996866\n",
      "clippling gradient: 20.0374105155 with coef 0.998132966558\n",
      "clippling gradient: 24.7778407161 with coef 0.807172837583\n",
      "12:18:54 Epoch=19 Loss=-1.8542\n",
      "clippling gradient: 62.9356708426 with coef 0.317784806807\n",
      "12:19:04 Epoch=20 Loss=-1.9261\n",
      "12:20:49 Epoch=20 Loss=-1.8584\n",
      "clippling gradient: 26.8401693642 with coef 0.745151780848\n",
      "12:22:34 Epoch=20 Loss=-1.8535\n",
      "clippling gradient: 27.6023867927 with coef 0.724575021364\n",
      "12:24:19 Epoch=20 Loss=-1.8541\n",
      "clippling gradient: 25.1199949887 with coef 0.796178502781\n",
      "12:26:04 Epoch=20 Loss=-1.8496\n",
      "Arousal CCC:  0.314489517626\n",
      "Arousal Cor:  0.330960405263\n",
      "Arousal MSE:  0.0511053895926\n",
      "Valence CCC:  0.458063494917\n",
      "Valence Cor:  0.469071996141\n",
      "Valence MSE:  0.102404684053\n",
      "Total CCC:    0.772553012543\n",
      "12:27:39 Epoch=21 Loss=-1.9449\n",
      "clippling gradient: 20.1276899083 with coef 0.993656007775\n",
      "clippling gradient: 21.5282491557 with coef 0.929011916172\n",
      "12:29:23 Epoch=21 Loss=-1.8503\n",
      "clippling gradient: 22.6129547486 with coef 0.884448769405\n",
      "clippling gradient: 38.8354048385 with coef 0.514993987655\n",
      "12:31:09 Epoch=21 Loss=-1.8446\n",
      "clippling gradient: 20.1957627503 with coef 0.990306741432\n",
      "12:32:54 Epoch=21 Loss=-1.8436\n",
      "clippling gradient: 28.4074360551 with coef 0.704041010994\n",
      "12:34:39 Epoch=21 Loss=-1.8429\n",
      "12:34:48 Epoch=22 Loss=-1.9256\n",
      "clippling gradient: 22.057026136 with coef 0.906740549551\n",
      "clippling gradient: 23.8362668417 with coef 0.839057564374\n",
      "12:36:33 Epoch=22 Loss=-1.8412\n",
      "clippling gradient: 20.3612527988 with coef 0.982257830481\n",
      "12:38:18 Epoch=22 Loss=-1.8512\n",
      "clippling gradient: 45.0055354668 with coef 0.444389779892\n",
      "clippling gradient: 21.37505961 with coef 0.935669905249\n",
      "12:40:03 Epoch=22 Loss=-1.8440\n",
      "12:41:49 Epoch=22 Loss=-1.8464\n",
      "12:41:56 Epoch=23 Loss=-1.9318\n",
      "clippling gradient: 22.270886865 with coef 0.898033388666\n",
      "12:43:42 Epoch=23 Loss=-1.8499\n",
      "clippling gradient: 20.4707044372 with coef 0.977005948248\n",
      "12:45:27 Epoch=23 Loss=-1.8507\n",
      "clippling gradient: 20.0677740662 with coef 0.996622741217\n",
      "clippling gradient: 21.1114407228 with coef 0.947353629847\n",
      "clippling gradient: 27.8528476659 with coef 0.718059432914\n",
      "12:47:12 Epoch=23 Loss=-1.8509\n",
      "12:48:57 Epoch=23 Loss=-1.8486\n",
      "Arousal CCC:  0.325021110275\n",
      "Arousal Cor:  0.341161052862\n",
      "Arousal MSE:  0.0503743729316\n",
      "Valence CCC:  0.456881158672\n",
      "Valence Cor:  0.468426410778\n",
      "Valence MSE:  0.102683913386\n",
      "Total CCC:    0.781902268947\n",
      "12:50:48 Epoch=24 Loss=-1.9341\n",
      "clippling gradient: 20.3710423963 with coef 0.981785792346\n",
      "12:52:32 Epoch=24 Loss=-1.8374\n",
      "clippling gradient: 28.9719404301 with coef 0.690323109295\n",
      "clippling gradient: 33.4409056193 with coef 0.598069927523\n",
      "12:54:17 Epoch=24 Loss=-1.8506\n",
      "12:56:02 Epoch=24 Loss=-1.8576\n",
      "clippling gradient: 21.9901817681 with coef 0.909496802296\n",
      "12:57:47 Epoch=24 Loss=-1.8551\n",
      "12:57:56 Epoch=25 Loss=-1.9280\n",
      "clippling gradient: 25.0803747363 with coef 0.797436250864\n",
      "clippling gradient: 23.3762189097 with coef 0.855570358801\n",
      "clippling gradient: 24.8769970057 with coef 0.803955557635\n",
      "12:59:41 Epoch=25 Loss=-1.8577\n",
      "13:01:26 Epoch=25 Loss=-1.8538\n",
      "clippling gradient: 35.0820187548 with coef 0.570092620376\n",
      "clippling gradient: 23.1370289423 with coef 0.864415221586\n",
      "clippling gradient: 22.6572670289 with coef 0.882718995831\n",
      "13:03:12 Epoch=25 Loss=-1.8485\n",
      "13:04:57 Epoch=25 Loss=-1.8483\n",
      "13:05:05 Epoch=26 Loss=-1.1067\n",
      "clippling gradient: 23.2126502442 with coef 0.861599162077\n",
      "clippling gradient: 23.7486912102 with coef 0.842151671557\n",
      "13:06:50 Epoch=26 Loss=-1.8381\n",
      "clippling gradient: 20.0257538507 with coef 0.998713963487\n",
      "clippling gradient: 31.0657198176 with coef 0.643796445646\n",
      "clippling gradient: 21.9500068845 with coef 0.911161445425\n",
      "13:08:35 Epoch=26 Loss=-1.8431\n",
      "clippling gradient: 26.1025656918 with coef 0.766208204825\n",
      "clippling gradient: 22.3537872493 with coef 0.894702977037\n",
      "13:10:21 Epoch=26 Loss=-1.8427\n",
      "clippling gradient: 26.6528742921 with coef 0.750388111271\n",
      "clippling gradient: 23.9559493876 with coef 0.834865681023\n",
      "13:12:06 Epoch=26 Loss=-1.8435\n",
      "Arousal CCC:  0.320965549647\n",
      "Arousal Cor:  0.33693062418\n",
      "Arousal MSE:  0.0506550998816\n",
      "Valence CCC:  0.46038122512\n",
      "Valence Cor:  0.471960863134\n",
      "Valence MSE:  0.102742624632\n",
      "Total CCC:    0.781346774768\n",
      "13:13:42 Epoch=27 Loss=-1.8103\n",
      "clippling gradient: 24.0125079796 with coef 0.832899254712\n",
      "clippling gradient: 20.4582001499 with coef 0.977603105524\n",
      "clippling gradient: 20.7867591357 with coef 0.962150947604\n",
      "13:15:27 Epoch=27 Loss=-1.8542\n",
      "clippling gradient: 34.1403347884 with coef 0.585817336705\n",
      "13:17:12 Epoch=27 Loss=-1.8509\n",
      "clippling gradient: 22.0705950977 with coef 0.90618308711\n",
      "clippling gradient: 22.3928396842 with coef 0.893142642113\n",
      "13:18:57 Epoch=27 Loss=-1.8544\n",
      "clippling gradient: 20.5682335803 with coef 0.972373243523\n",
      "clippling gradient: 23.3884897885 with coef 0.855121479875\n",
      "13:20:43 Epoch=27 Loss=-1.8577\n",
      "13:20:51 Epoch=28 Loss=-1.6347\n",
      "clippling gradient: 22.9429924166 with coef 0.871725868921\n",
      "13:22:37 Epoch=28 Loss=-1.8555\n",
      "13:24:22 Epoch=28 Loss=-1.8527\n",
      "clippling gradient: 23.0127787334 with coef 0.869082357748\n",
      "clippling gradient: 30.6939335247 with coef 0.651594556427\n",
      "clippling gradient: 41.63929049 with coef 0.480315580901\n",
      "13:26:07 Epoch=28 Loss=-1.8524\n",
      "clippling gradient: 44.7028184269 with coef 0.447399083633\n",
      "13:27:53 Epoch=28 Loss=-1.8561\n",
      "13:28:01 Epoch=29 Loss=-1.9617\n",
      "13:29:46 Epoch=29 Loss=-1.8554\n",
      "clippling gradient: 20.7192509535 with coef 0.965285861197\n",
      "clippling gradient: 20.8196377789 with coef 0.960631506293\n",
      "13:31:32 Epoch=29 Loss=-1.8646\n",
      "clippling gradient: 27.8145537587 with coef 0.719048026925\n",
      "13:33:17 Epoch=29 Loss=-1.8574\n",
      "13:35:02 Epoch=29 Loss=-1.8557\n",
      "Arousal CCC:  0.320179578731\n",
      "Arousal Cor:  0.335798606417\n",
      "Arousal MSE:  0.0507318803466\n",
      "Valence CCC:  0.451716196233\n",
      "Valence Cor:  0.462853285972\n",
      "Valence MSE:  0.104399975576\n",
      "Total CCC:    0.771895774964\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)    \n",
    "best_arou_ccc, best_vale_ccc = validate(val_loader, model, 0)\n",
    "for epoch in range(n_epoch):\n",
    "    if epoch in lr_steps:\n",
    "        lr *= 0.1\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)    \n",
    "\n",
    "    train(train_loader, model, criterion, optimizer, epoch)\n",
    "#     save_model(model, './results/epoch_lstm_{}.pth'.format(epoch))\n",
    "    \n",
    "    # evaluate on validation set\n",
    "    if (epoch+1)%eval_freq == 0 or epoch == n_epoch-1:\n",
    "        arou_ccc, vale_ccc = validate(val_loader, model, epoch)\n",
    "        \n",
    "        if (arou_ccc+vale_ccc) > (best_arou_ccc + best_vale_ccc):\n",
    "            best_arou_ccc = arou_ccc\n",
    "            best_vale_ccc = vale_ccc\n",
    "            save_model(model,'./pth/joint_ccc2_{}_{}_{}.pth'.format(epoch, round(arou_ccc,4), round(vale_ccc,4)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
