{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "from sys import path\n",
    "from os.path import join as join_paths\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from numpy.random import randint\n",
    "from skimage import io\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "path.append(\"./\")\n",
    "import net_sphere\n",
    "from calculateEvaluationCCC import calculateCCC\n",
    "\n",
    "# Define parameters\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "lr = 0.01\n",
    "bs = 32\n",
    "n_epoch = 30\n",
    "lr_steps = [8,16,24]\n",
    "\n",
    "gd = 20 # clip gradient\n",
    "eval_freq = 3\n",
    "print_freq = 20\n",
    "num_worker = 4\n",
    "num_seg = 16\n",
    "flag_biLSTM = True\n",
    "\n",
    "classnum = 7\n",
    "\n",
    "train_list_path = './support_tables/train_list_lstm.txt'\n",
    "val_list_path = './support_tables/validation_list_lstm.txt'\n",
    "model_path = './model/sphere20a_20171020.pth'\n",
    "train_data_path: str = \"/Users/leonardoalchieri/Datasets/OMGEmotionChallenge/Train_Set/trimmed_faces\"\n",
    "validation_data_path: str = \"/Users/leonardoalchieri/Datasets/OMGEmotionChallenge/Validation_Set/trimmed_faces\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sphereface = getattr(net_sphere,'sphere20a')()\n",
    "sphereface.load_state_dict(torch.load(model_path))\n",
    "sphereface.feature = True # remove the last fc layer because we need to use LSTM first\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, sphereface):\n",
    "        super(Net, self).__init__()\n",
    "        self.sphereface = sphereface\n",
    "        self.linear = torch.nn.Linear(512,2)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.avgPool = torch.nn.AvgPool2d((num_seg,1), stride=1)\n",
    "        self.LSTM = torch.nn.LSTM(512, 512, 1, batch_first = True, dropout=0.2, bidirectional=flag_biLSTM)  # Input dim, hidden dim, num_layer\n",
    "        for name, param in self.LSTM.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                torch.nn.init.constant(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                torch.nn.init.orthogonal(param)\n",
    "        \n",
    "    def sequentialLSTM(self, input, hidden=None):\n",
    "\n",
    "        input_lstm = input.view([-1,num_seg, input.shape[1]])\n",
    "        batch_size = input_lstm.shape[0]\n",
    "        feature_size = input_lstm.shape[2]\n",
    "\n",
    "        self.LSTM.flatten_parameters()\n",
    "            \n",
    "        output_lstm, hidden = self.LSTM(input_lstm)\n",
    "        if flag_biLSTM:\n",
    "             output_lstm = output_lstm.contiguous().view(batch_size, output_lstm.size(1), 2, -1).sum(2).view(batch_size, output_lstm.size(1), -1) \n",
    "\n",
    "        # avarage the output of LSTM\n",
    "        output_lstm = output_lstm.view(batch_size,1,num_seg,-1)\n",
    "        out = self.avgPool(output_lstm)\n",
    "        out = out.view(batch_size,-1)\n",
    "        return out\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.sphereface(x)\n",
    "        x = self.sequentialLSTM(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.tanh(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printoneline(*argv):\n",
    "    s = ''\n",
    "    for arg in argv: s += str(arg) + ' '\n",
    "    s = s[:-1]\n",
    "    sys.stdout.write('\\r'+s)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def dt():\n",
    "    return datetime.datetime.now().strftime('%H:%M:%S')\n",
    "\n",
    "def save_model(model,filename):\n",
    "    state = model.state_dict()\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonardoalchieri/miniconda3/envs/torch_latest/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/var/folders/rc/t3h_b88s3vbg5dcd4pnlgskr0000gp/T/ipykernel_37342/282168897.py:17: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
      "  torch.nn.init.orthogonal(param)\n",
      "/var/folders/rc/t3h_b88s3vbg5dcd4pnlgskr0000gp/T/ipykernel_37342/282168897.py:15: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  torch.nn.init.constant(param, 0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data loaders\n",
      "Loading output file\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a417d3111243b5ab570016dbff7cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation batch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/leonardoalchieri/miniconda3/envs/torch_latest/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/leonardoalchieri/miniconda3/envs/torch_latest/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'OMGDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/leonardoalchieri/Desktop/GIT/OMG-ADSC/trainVal_Video_LSTM.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 150>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bmacstudio/Users/leonardoalchieri/Desktop/GIT/OMG-ADSC/trainVal_Video_LSTM.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=142'>143</a>\u001b[0m val_loader \u001b[39m=\u001b[39m DataLoader(OMGDataset(val_list_path,validation_data_path), \n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bmacstudio/Users/leonardoalchieri/Desktop/GIT/OMG-ADSC/trainVal_Video_LSTM.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=143'>144</a>\u001b[0m                         batch_size\u001b[39m=\u001b[39mbs, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bmacstudio/Users/leonardoalchieri/Desktop/GIT/OMG-ADSC/trainVal_Video_LSTM.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=147'>148</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mSGD(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlr, momentum\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m, weight_decay\u001b[39m=\u001b[39m\u001b[39m5e-4\u001b[39m)    \n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bmacstudio/Users/leonardoalchieri/Desktop/GIT/OMG-ADSC/trainVal_Video_LSTM.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=149'>150</a>\u001b[0m best_arou_ccc, best_vale_ccc \u001b[39m=\u001b[39m validate(val_loader, model, criterion,\u001b[39m0\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bmacstudio/Users/leonardoalchieri/Desktop/GIT/OMG-ADSC/trainVal_Video_LSTM.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=151'>152</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(n_epoch), desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bmacstudio/Users/leonardoalchieri/Desktop/GIT/OMG-ADSC/trainVal_Video_LSTM.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=152'>153</a>\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39min\u001b[39;00m lr_steps:\n",
      "\u001b[1;32m/Users/leonardoalchieri/Desktop/GIT/OMG-ADSC/trainVal_Video_LSTM.ipynb Cell 4\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(val_loader, model, criterion, epoch)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmacstudio/Users/leonardoalchieri/Desktop/GIT/OMG-ADSC/trainVal_Video_LSTM.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m txt_result \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mresults/val_lstm_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%\u001b[39mepoch, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmacstudio/Users/leonardoalchieri/Desktop/GIT/OMG-ADSC/trainVal_Video_LSTM.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m txt_result\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39mvideo,utterance,arousal,valence\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmacstudio/Users/leonardoalchieri/Desktop/GIT/OMG-ADSC/trainVal_Video_LSTM.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mfor\u001b[39;00m (inputs, targets,(vid, utter)) \u001b[39min\u001b[39;00m tqdm(val_loader, \u001b[39m'\u001b[39m\u001b[39mValidation batch\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmacstudio/Users/leonardoalchieri/Desktop/GIT/OMG-ADSC/trainVal_Video_LSTM.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m     \u001b[39mif\u001b[39;00m use_cuda:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmacstudio/Users/leonardoalchieri/Desktop/GIT/OMG-ADSC/trainVal_Video_LSTM.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m         inputs, targets \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mcuda(), targets\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_latest/lib/python3.10/site-packages/tqdm/notebook.py:258\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[0;32m--> 258\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[1;32m    259\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    260\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m    261\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_latest/lib/python3.10/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_latest/lib/python3.10/site-packages/torch/utils/data/dataloader.py:435\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[1;32m    434\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 435\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_latest/lib/python3.10/site-packages/torch/utils/data/dataloader.py:381\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    380\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 381\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_latest/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1034\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1027\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1034\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[1;32m   1035\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1036\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_latest/lib/python3.10/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_latest/lib/python3.10/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_latest/lib/python3.10/multiprocessing/context.py:284\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    283\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_posix\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[0;32m--> 284\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_latest/lib/python3.10/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fds \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(process_obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_latest/lib/python3.10/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalizer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_launch(process_obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_latest/lib/python3.10/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentinel \u001b[39m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(parent_w, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m, closefd\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         f\u001b[39m.\u001b[39;49mwrite(fp\u001b[39m.\u001b[39;49mgetbuffer())\n\u001b[1;32m     63\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batch_idx = 0\n",
    "    \n",
    "    for i, (inputs, targets, _) in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda(non_blocking=True)\n",
    "\n",
    "        inputs = torch.autograd.Variable(inputs)\n",
    "        targets = torch.autograd.Variable(targets)\n",
    "        \n",
    "        inputs = inputs.view((-1,3)+inputs.size()[-2:])\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        \n",
    "        loss = criterion(outputs,targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #tsn uses clipping gradient\n",
    "        if gd is not None:\n",
    "            total_norm = clip_grad_norm(model.parameters(),gd)\n",
    "            if total_norm > gd:\n",
    "                print('clippling gradient: {} with coef {}'.format(total_norm, gd/total_norm))\n",
    "                \n",
    "        train_loss += loss.data[0]\n",
    "        \n",
    "        if i % print_freq == 0:\n",
    "            printoneline(dt(),'Epoch=%d Loss=%.4f\\n'\n",
    "                % (epoch,train_loss/(batch_idx+1)))\n",
    "        batch_idx += 1\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    err_arou = 0.0\n",
    "    err_vale = 0.0\n",
    "    \n",
    "    print('Loading output file')\n",
    "    txt_result = open('results/val_lstm_%d.csv'%epoch, 'w')\n",
    "    txt_result.write('video,utterance,arousal,valence\\n')\n",
    "    for (inputs, targets,(vid, utter)) in tqdm(val_loader, 'Validation batch'):\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        \n",
    "        inputs = torch.autograd.Variable(inputs)\n",
    "        targets = torch.autograd.Variable(targets)\n",
    "        \n",
    "        inputs = inputs.view((-1,3)+inputs.size()[-2:])\n",
    "        print(f'Getting model output')\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        outputs = outputs.data.cpu().numpy()\n",
    "        targets = targets.data.cpu().numpy()\n",
    "        \n",
    "        err_arou += np.sum((outputs[:,0]-targets[:,0])**2)\n",
    "        err_vale += np.sum((outputs[:,1]-targets[:,1])**2)\n",
    "        \n",
    "        print(\n",
    "            'preparing to write ccc results'\n",
    "        )\n",
    "        for i in range(len(vid)):\n",
    "            out = outputs\n",
    "            txt_result.write('%s,%s.mp4,%f,%f\\n'%(vid[i], utter[i],out[i][0],out[i][1]))\n",
    "    \n",
    "    txt_result.close()\n",
    "    \n",
    "    arouCCC, valeCCC = calculateCCC('./results/omg_ValidationVideos.csv','results/val_lstm_%d.csv'%epoch)\n",
    "    return (arouCCC,valeCCC)\n",
    "\n",
    "class OMGDataset(Dataset):\n",
    "    \"\"\"OMG dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, txt_file, base_path, transform=None):\n",
    "        self.base_path = base_path\n",
    "        self.data = pd.read_csv(txt_file, sep=\" \", header=0, index_col=0)\n",
    "        self.data.dropna(inplace=True, how='any')\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vid = self.data.iloc[idx,0]\n",
    "        utter = self.data.iloc[idx,1]\n",
    "        img_list = self.data.iloc[idx,-1]\n",
    "        img_list = img_list.split(',')[:-1]\n",
    "        # img_list = [int(img) for img in img_list]\n",
    "        print('Prepared imaged ids')\n",
    "        \n",
    "        num_frames = len(img_list)\n",
    "        # inspired by TSN's pytorch code\n",
    "        # FIXME: num_seg is hardcoded\n",
    "        average_duration = num_frames // num_seg\n",
    "        if num_frames>num_seg:\n",
    "            offsets = np.multiply(list(range(num_seg)), average_duration) + randint(average_duration, size=num_seg)\n",
    "        else:\n",
    "            tick = num_frames / float(num_seg)\n",
    "            offsets = np.array([int(tick / 2.0 + tick * x) for x in range(num_seg)])\n",
    "        print('Created offests')\n",
    "\n",
    "        final_list = [img_list[i] for i in offsets]\n",
    "        \n",
    "        # stack images within a video in the depth dimension\n",
    "        print('Stacking images')\n",
    "        for i,ind in enumerate(final_list):\n",
    "            image = io.imread(join_paths(self.base_path,'%s/%s/%s.png'%(vid,utter,ind))).astype(np.float32)\n",
    "            image = torch.from_numpy(((image - 127.5)/128).transpose(2,0,1))\n",
    "            if i==0:\n",
    "                images = image\n",
    "            else:\n",
    "                images = torch.cat((images,image), 0)\n",
    "        print('Stacked images')\n",
    "        \n",
    "        label = torch.from_numpy(np.array([self.data.iloc[idx,2], self.data.iloc[idx,3]]).astype(np.float32))\n",
    "        print('Prepared labels')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return (images, label, (vid,utter))\n",
    "    \n",
    "    \n",
    "\n",
    "model = Net(sphereface)\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "print('Preparing data loaders')\n",
    "train_loader = DataLoader(OMGDataset(train_list_path,train_data_path), \n",
    "                          batch_size=bs, shuffle=True, num_workers=1)\n",
    "val_loader = DataLoader(OMGDataset(val_list_path,validation_data_path), \n",
    "                        batch_size=bs, shuffle=False, num_workers=1)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)    \n",
    "\n",
    "best_arou_ccc, best_vale_ccc = validate(val_loader, model, criterion,0)\n",
    "\n",
    "for epoch in tqdm(range(n_epoch), desc='Epoch'):\n",
    "    if epoch in lr_steps:\n",
    "        lr *= 0.1\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)    \n",
    "\n",
    "    train(train_loader, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    # evaluate on validation set\n",
    "    if (epoch+1)%eval_freq == 0 or epoch == n_epoch-1:\n",
    "        arou_ccc, vale_ccc = validate(val_loader, model, criterion,epoch)\n",
    "        \n",
    "        if (arou_ccc+vale_ccc) > (best_arou_ccc + best_vale_ccc):\n",
    "            best_arou_ccc = arou_ccc\n",
    "            best_vale_ccc = vale_ccc\n",
    "            save_model(model,'./pth/model_lstm_{}_{}_{}.pth'.format(epoch, round(arou_ccc,4), round(vale_ccc,4)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('torch_latest')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "fcbe6412c233032cd4d98c2bce6ac3e1dc35813c70919b7ccf65f269cc451245"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
