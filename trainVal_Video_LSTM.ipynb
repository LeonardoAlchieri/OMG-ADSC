{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import io\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import net_sphere\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import datetime,sys\n",
    "from numpy.random import randint\n",
    "from calculateEvaluationCCC import calculateCCC\n",
    "\n",
    "# Define parameters\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# lr = 0.01\n",
    "# bs = 32\n",
    "# n_epoch = 20\n",
    "# lr_steps = [7,14]\n",
    "lr = 0.01\n",
    "bs = 32\n",
    "n_epoch = 30\n",
    "lr_steps = [8,16,24]\n",
    "\n",
    "gd = 20 # clip gradient\n",
    "eval_freq = 3\n",
    "print_freq = 20\n",
    "num_worker = 4\n",
    "num_seg = 16\n",
    "flag_biLSTM = True\n",
    "\n",
    "classnum = 7\n",
    "\n",
    "train_list_path = './data/OMG_Aligned/train_list_lstm.txt'\n",
    "val_list_path = './data/OMG_Aligned/val_list_lstm.txt'\n",
    "model_path = './model/sphere20a_20171020.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sphereface = getattr(net_sphere,'sphere20a')()\n",
    "sphereface.load_state_dict(torch.load(model_path))\n",
    "sphereface.feature = True # remove the last fc layer because we need to use LSTM first\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, sphereface):\n",
    "        super(Net, self).__init__()\n",
    "        self.sphereface = sphereface\n",
    "        self.linear = torch.nn.Linear(512,2)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.avgPool = torch.nn.AvgPool2d((num_seg,1), stride=1)\n",
    "        self.LSTM = torch.nn.LSTM(512, 512, 1, batch_first = True, dropout=0.2, bidirectional=flag_biLSTM)  # Input dim, hidden dim, num_layer\n",
    "        for name, param in self.LSTM.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                torch.nn.init.constant(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                torch.nn.init.orthogonal(param)\n",
    "        \n",
    "    def sequentialLSTM(self, input, hidden=None):\n",
    "\n",
    "        input_lstm = input.view([-1,num_seg, input.shape[1]])\n",
    "        batch_size = input_lstm.shape[0]\n",
    "        feature_size = input_lstm.shape[2]\n",
    "\n",
    "        self.LSTM.flatten_parameters()\n",
    "            \n",
    "        output_lstm, hidden = self.LSTM(input_lstm)\n",
    "        if flag_biLSTM:\n",
    "             output_lstm = output_lstm.contiguous().view(batch_size, output_lstm.size(1), 2, -1).sum(2).view(batch_size, output_lstm.size(1), -1) \n",
    "\n",
    "        # avarage the output of LSTM\n",
    "#         print '0', output_lstm.shape\n",
    "        output_lstm = output_lstm.view(batch_size,1,num_seg,-1)\n",
    "#         print '1',output_lstm.shape\n",
    "        out = self.avgPool(output_lstm)\n",
    "#         print '2',out.shape\n",
    "        out = out.view(batch_size,-1)\n",
    "        return out\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.sphereface(x)\n",
    "        x = self.sequentialLSTM(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.tanh(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = Net(sphereface)\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "    \n",
    "def \n",
    "\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OMGDataset(Dataset):\n",
    "    \"\"\"OMG dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, txt_file, base_path, transform=None):\n",
    "        self.base_path = base_path\n",
    "        self.data = pd.read_csv(txt_file, sep=\" \", header=None)\n",
    "#         self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vid = self.data.iloc[idx,0]\n",
    "        utter = self.data.iloc[idx,1]\n",
    "        img_list = self.data.iloc[idx,-1]\n",
    "        img_list = img_list.split(',')[:-1]\n",
    "        img_list = map(int, img_list)\n",
    "        \n",
    "        num_frames = len(img_list)\n",
    "        # inspired by TSN's pytorch code\n",
    "        average_duration = num_frames // num_seg\n",
    "        if num_frames>num_seg:\n",
    "            offsets = np.multiply(list(range(num_seg)), average_duration) + randint(average_duration, size=num_seg)\n",
    "        else:\n",
    "            tick = num_frames / float(num_seg)\n",
    "            offsets = np.array([int(tick / 2.0 + tick * x) for x in range(num_seg)])\n",
    "\n",
    "        final_list = [img_list[i] for i in offsets]\n",
    "        \n",
    "        # stack images within a video in the depth dimension\n",
    "        for i,ind in enumerate(final_list):\n",
    "            image = io.imread(self.base_path+'%s/%s/%d.jpg'%(vid,utter,ind)).astype(np.float32)\n",
    "            image = torch.from_numpy(((image - 127.5)/128).transpose(2,0,1))\n",
    "            if i==0:\n",
    "                images = image\n",
    "            else:\n",
    "                images = torch.cat((images,image), 0)\n",
    "        \n",
    "        \n",
    "        label = torch.from_numpy(np.array([self.data.iloc[idx,2], self.data.iloc[idx,3]]).astype(np.float32))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return (images, label, (vid,utter))\n",
    "    \n",
    "train_loader = DataLoader(OMGDataset(train_list_path,'./data/OMG_Aligned/Train/'), \n",
    "                          batch_size=bs, shuffle=True, num_workers=num_worker)\n",
    "val_loader = DataLoader(OMGDataset(val_list_path,'./data/OMG_Aligned/Val/'), \n",
    "                        batch_size=bs, shuffle=False, num_workers=num_worker)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printoneline(*argv):\n",
    "    s = ''\n",
    "    for arg in argv: s += str(arg) + ' '\n",
    "    s = s[:-1]\n",
    "    sys.stdout.write('\\r'+s)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def dt():\n",
    "    return datetime.datetime.now().strftime('%H:%M:%S')\n",
    "\n",
    "def save_model(model,filename):\n",
    "    state = model.state_dict()\n",
    "#     for key in state: state[key] = state[key].clone().cpu()\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batch_idx = 0\n",
    "    \n",
    "    for i, (inputs, targets, _) in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda(async=True)\n",
    "\n",
    "        inputs = torch.autograd.Variable(inputs)\n",
    "        targets = torch.autograd.Variable(targets)\n",
    "        \n",
    "        inputs = inputs.view((-1,3)+inputs.size()[-2:])\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "#         print 'here',outputs.shape, targets.shape\n",
    "        \n",
    "        loss = criterion(outputs,targets)\n",
    "        \n",
    "#         lossd = loss.data[0]        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #tsn uses clipping gradient\n",
    "        if gd is not None:\n",
    "            total_norm = clip_grad_norm(model.parameters(),gd)\n",
    "            if total_norm > gd:\n",
    "                print('clippling gradient: {} with coef {}'.format(total_norm, gd/total_norm))\n",
    "                \n",
    "        train_loss += loss.data[0]\n",
    "        \n",
    "        if i % print_freq == 0:\n",
    "            printoneline(dt(),'Epoch=%d Loss=%.4f\\n'\n",
    "                % (epoch,train_loss/(batch_idx+1)))\n",
    "        batch_idx += 1\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    err_arou = 0.0\n",
    "    err_vale = 0.0\n",
    "    \n",
    "    txt_result = open('results/val_lstm_%d.csv'%epoch, 'w')\n",
    "    txt_result.write('video,utterance,arousal,valence\\n')\n",
    "    for (inputs, targets,(vid, utter)) in val_loader:\n",
    "#         print inputs.shape\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        \n",
    "        inputs = torch.autograd.Variable(inputs)\n",
    "        targets = torch.autograd.Variable(targets)\n",
    "        \n",
    "        inputs = inputs.view((-1,3)+inputs.size()[-2:])\n",
    "#         print inputs.size()[-2:]\n",
    "#         print inputs.shape\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "#         print outputs[:,0]-targets[:,0]\n",
    "        outputs = outputs.data.cpu().numpy()\n",
    "        targets = targets.data.cpu().numpy()\n",
    "        \n",
    "#         print outputs, targets\n",
    "        err_arou += np.sum((outputs[:,0]-targets[:,0])**2)\n",
    "        err_vale += np.sum((outputs[:,1]-targets[:,1])**2)\n",
    "        \n",
    "        for i in range(len(vid)):\n",
    "#             name = img_name[i].replace('/home/m2a03/Work/sphereface_pytorch/data/OMG_Aligned/', '')\n",
    "            out = outputs\n",
    "#             print vid[0], utter[0]\n",
    "            txt_result.write('%s,%s.mp4,%f,%f\\n'%(vid[i], utter[i],out[i][0],out[i][1]))\n",
    "    \n",
    "#     print('MSE of arousal: %f' % (err_arou / len(val_loader.dataset)))\n",
    "#     print('MSE of valence: %f' % (err_vale / len(val_loader.dataset)))\n",
    "#     print('MSE of total: %f' % ((err_arou + err_vale) / len(val_loader.dataset)))\n",
    "    txt_result.close()\n",
    "    \n",
    "    arouCCC, valeCCC = calculateCCC('./results/omg_ValidationVideos.csv','results/val_lstm_%d.csv'%epoch)\n",
    "    return (arouCCC,valeCCC)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# best_mse = validate(val_loader, model, criterion, 0)\n",
    "\n",
    "# print best_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arousal CCC:  -0.006611498662120568\n",
      "Arousal Cor:  -0.03837046335237764\n",
      "Arousal MSE:  0.342062354754905\n",
      "Valence CCC:  0.009066932179317127\n",
      "Valence Cor:  0.015659614804120568\n",
      "Valence MSE:  0.20248923395417961\n",
      "Total CCC:    0.002455433517196559\n",
      "10:51:27 Epoch=0 Loss=0.2141\n",
      "10:52:16 Epoch=0 Loss=0.1206\n",
      "10:53:06 Epoch=0 Loss=0.0961\n",
      "10:53:55 Epoch=0 Loss=0.0840\n",
      "10:54:36 Epoch=1 Loss=0.0661\n",
      "10:55:26 Epoch=1 Loss=0.0513\n",
      "10:56:15 Epoch=1 Loss=0.0497\n",
      "10:57:05 Epoch=1 Loss=0.0486\n",
      "10:57:46 Epoch=2 Loss=0.0303\n",
      "10:58:35 Epoch=2 Loss=0.0403\n",
      "10:59:25 Epoch=2 Loss=0.0418\n",
      "11:00:14 Epoch=2 Loss=0.0410\n",
      "Arousal CCC:  0.1992311863391338\n",
      "Arousal Cor:  0.23305603583524825\n",
      "Arousal MSE:  0.051668559858100746\n",
      "Valence CCC:  0.3640836374604136\n",
      "Valence Cor:  0.3789696962662488\n",
      "Valence MSE:  0.11537642154812851\n",
      "Total CCC:    0.5633148237995474\n",
      "11:01:10 Epoch=3 Loss=0.0294\n",
      "11:02:00 Epoch=3 Loss=0.0370\n",
      "11:02:49 Epoch=3 Loss=0.0365\n",
      "11:03:39 Epoch=3 Loss=0.0377\n",
      "11:04:19 Epoch=4 Loss=0.0411\n",
      "11:05:09 Epoch=4 Loss=0.0335\n",
      "11:05:58 Epoch=4 Loss=0.0340\n",
      "11:06:48 Epoch=4 Loss=0.0345\n",
      "11:07:29 Epoch=5 Loss=0.0551\n",
      "11:08:18 Epoch=5 Loss=0.0308\n",
      "11:09:08 Epoch=5 Loss=0.0319\n",
      "11:09:57 Epoch=5 Loss=0.0320\n",
      "Arousal CCC:  0.2276984165006668\n",
      "Arousal Cor:  0.235670984459041\n",
      "Arousal MSE:  0.05729716690246082\n",
      "Valence CCC:  0.3805831867406859\n",
      "Valence Cor:  0.39032596923901147\n",
      "Valence MSE:  0.11532505180130896\n",
      "Total CCC:    0.6082816032413527\n",
      "11:10:54 Epoch=6 Loss=0.0287\n",
      "11:11:43 Epoch=6 Loss=0.0312\n",
      "11:12:33 Epoch=6 Loss=0.0300\n",
      "11:13:22 Epoch=6 Loss=0.0305\n",
      "11:14:03 Epoch=7 Loss=0.0238\n",
      "11:14:52 Epoch=7 Loss=0.0282\n",
      "11:15:42 Epoch=7 Loss=0.0299\n",
      "11:16:31 Epoch=7 Loss=0.0303\n",
      "11:17:12 Epoch=8 Loss=0.0301\n",
      "11:18:02 Epoch=8 Loss=0.0272\n",
      "11:18:51 Epoch=8 Loss=0.0255\n",
      "11:19:41 Epoch=8 Loss=0.0250\n",
      "Arousal CCC:  0.2321669216141064\n",
      "Arousal Cor:  0.24574492836312095\n",
      "Arousal MSE:  0.05531369070108905\n",
      "Valence CCC:  0.3883493339267687\n",
      "Valence Cor:  0.4000196879243058\n",
      "Valence MSE:  0.11555356638518212\n",
      "Total CCC:    0.6205162555408751\n",
      "11:20:37 Epoch=9 Loss=0.0153\n",
      "11:21:27 Epoch=9 Loss=0.0227\n",
      "11:22:16 Epoch=9 Loss=0.0227\n",
      "11:23:06 Epoch=9 Loss=0.0242\n",
      "11:23:46 Epoch=10 Loss=0.0289\n",
      "11:24:36 Epoch=10 Loss=0.0233\n",
      "11:25:25 Epoch=10 Loss=0.0237\n",
      "11:26:15 Epoch=10 Loss=0.0235\n",
      "11:26:56 Epoch=11 Loss=0.0223\n",
      "11:27:45 Epoch=11 Loss=0.0246\n",
      "11:28:35 Epoch=11 Loss=0.0235\n",
      "11:29:24 Epoch=11 Loss=0.0234\n",
      "Arousal CCC:  0.2338839098871367\n",
      "Arousal Cor:  0.2454933032144747\n",
      "Arousal MSE:  0.05604220945057635\n",
      "Valence CCC:  0.3919421041732282\n",
      "Valence Cor:  0.4006966467692443\n",
      "Valence MSE:  0.11770043112549104\n",
      "Total CCC:    0.6258260140603649\n",
      "11:30:20 Epoch=12 Loss=0.0339\n",
      "11:31:10 Epoch=12 Loss=0.0241\n",
      "11:31:59 Epoch=12 Loss=0.0243\n",
      "11:32:49 Epoch=12 Loss=0.0235\n",
      "11:33:30 Epoch=13 Loss=0.0215\n",
      "11:34:19 Epoch=13 Loss=0.0222\n",
      "11:35:09 Epoch=13 Loss=0.0230\n",
      "11:35:58 Epoch=13 Loss=0.0224\n",
      "11:36:39 Epoch=14 Loss=0.0199\n",
      "11:37:28 Epoch=14 Loss=0.0216\n",
      "11:38:18 Epoch=14 Loss=0.0217\n",
      "11:39:07 Epoch=14 Loss=0.0230\n",
      "Arousal CCC:  0.2371771789955678\n",
      "Arousal Cor:  0.24753248180781667\n",
      "Arousal MSE:  0.056463057234093415\n",
      "Valence CCC:  0.3934595540218911\n",
      "Valence Cor:  0.40378774111173343\n",
      "Valence MSE:  0.11785306686197417\n",
      "Total CCC:    0.6306367330174589\n",
      "11:40:04 Epoch=15 Loss=0.0351\n",
      "11:40:53 Epoch=15 Loss=0.0240\n",
      "11:41:43 Epoch=15 Loss=0.0229\n",
      "11:42:32 Epoch=15 Loss=0.0226\n",
      "11:43:13 Epoch=16 Loss=0.0206\n",
      "11:44:03 Epoch=16 Loss=0.0195\n",
      "11:44:52 Epoch=16 Loss=0.0209\n",
      "11:45:42 Epoch=16 Loss=0.0222\n",
      "11:46:23 Epoch=17 Loss=0.0202\n",
      "11:47:12 Epoch=17 Loss=0.0206\n",
      "11:48:02 Epoch=17 Loss=0.0220\n",
      "11:48:51 Epoch=17 Loss=0.0217\n",
      "Arousal CCC:  0.24314833163320035\n",
      "Arousal Cor:  0.25516544992490936\n",
      "Arousal MSE:  0.05561711117409095\n",
      "Valence CCC:  0.4030542829333696\n",
      "Valence Cor:  0.41024884323530997\n",
      "Valence MSE:  0.11635911291323595\n",
      "Total CCC:    0.6462026145665699\n",
      "11:49:47 Epoch=18 Loss=0.0167\n",
      "11:50:37 Epoch=18 Loss=0.0213\n",
      "11:51:26 Epoch=18 Loss=0.0220\n",
      "11:52:16 Epoch=18 Loss=0.0222\n",
      "11:52:56 Epoch=19 Loss=0.0221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-106:\n",
      "Process Process-107:\n",
      "Process Process-105:\n",
      "Process Process-108:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/m2a03/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "Traceback (most recent call last):\n",
      "    self.run()\n",
      "  File \"/home/m2a03/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "  File \"/home/m2a03/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/m2a03/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "  File \"/home/m2a03/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "  File \"/home/m2a03/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    r = index_queue.get()\n",
      "    self.run()\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/m2a03/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "  File \"/home/m2a03/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/home/m2a03/anaconda2/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "    self.run()\n",
      "  File \"/home/m2a03/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    return recv()\n",
      "  File \"/home/m2a03/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    r = index_queue.get()\n",
      "  File \"/home/m2a03/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "  File \"/home/m2a03/anaconda2/lib/python2.7/site-packages/torch/multiprocessing/queue.py\", line 21, in recv\n",
      "    r = index_queue.get()\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/m2a03/anaconda2/lib/python2.7/multiprocessing/queues.py\", line 374, in get\n",
      "    buf = self.recv_bytes()\n",
      "  File \"/home/m2a03/anaconda2/lib/python2.7/multiprocessing/queues.py\", line 374, in get\n",
      "  File \"/home/m2a03/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "KeyboardInterrupt\n",
      "    r = index_queue.get()\n",
      "  File \"/home/m2a03/anaconda2/lib/python2.7/multiprocessing/queues.py\", line 374, in get\n",
      "    racquire()\n",
      "    racquire()\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4b9834e5d0ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#     save_model(model, './results/epoch_lstm_{}.pth'.format(epoch))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-963cab76ff28>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#         lossd = loss.data[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/m2a03/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/m2a03/anaconda2/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)    \n",
    "best_arou_ccc, best_vale_ccc = validate(val_loader, model, criterion,0)\n",
    "for epoch in range(n_epoch):\n",
    "    if epoch in lr_steps:\n",
    "        lr *= 0.1\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)    \n",
    "\n",
    "    train(train_loader, model, criterion, optimizer, epoch)\n",
    "#     save_model(model, './results/epoch_lstm_{}.pth'.format(epoch))\n",
    "    \n",
    "    # evaluate on validation set\n",
    "    if (epoch+1)%eval_freq == 0 or epoch == n_epoch-1:\n",
    "        arou_ccc, vale_ccc = validate(val_loader, model, criterion,epoch)\n",
    "        \n",
    "        if (arou_ccc+vale_ccc) > (best_arou_ccc + best_vale_ccc):\n",
    "            best_arou_ccc = arou_ccc\n",
    "            best_vale_ccc = vale_ccc\n",
    "            save_model(model,'./pth/model_lstm_{}_{}_{}.pth'.format(epoch, round(arou_ccc,4), round(vale_ccc,4)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# t = '/home/m2a03/Work/sphereface_pytorch/data/OMG_Aligned/Val/74de88564/utterance_1/0.jpg'\n",
    "# tt = t.replace('/home/m2a03/Work/sphereface_pytorch/data/OMG_Aligned/','')\n",
    "# print tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
